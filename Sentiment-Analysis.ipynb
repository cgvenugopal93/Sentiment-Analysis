{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data files \n",
    "g = open('../pa2/reviews.txt','r') # What we know!\n",
    "reviews_all = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "g = open('../pa2/labels.txt','r') # What we WANT to know!\n",
    "sentiments_all = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()\n",
    "\n",
    "# load vocabulary\n",
    "g = open('../pa2/vocab.txt','r')\n",
    "vocab = [s.strip() for s in g.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a set of 25000 movie reviews, along with a `POSITIVE` or `NEGATIVE` sentiment label assigned to the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test data\n",
    "reviews_train,reviews_test = reviews_all[0:24000],reviews_all[24000:]\n",
    "sentiments_train,sentiments_test = sentiments_all[0:24000],sentiments_all[24000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintain Counter objects to store positive, negative and total counts for\n",
    "# all the words present in the positive, negative and total reviews.\n",
    "positive_word_count = Counter()\n",
    "negative_word_count = Counter()\n",
    "total_counts = Counter()\n",
    "\n",
    "# Instantiate Vocab as a <Key,None> Dict\n",
    "g = open('../pa2/vocab.txt','r')\n",
    "\n",
    "vocab_words_all = list(map(lambda x:x[:-1], g.readlines()))\n",
    "vocab_dict = dict(zip(vocab_words_all, vocab_words_all))\n",
    "selector = '[^A-Za-z]+'\n",
    "\n",
    "for index, review in enumerate(reviews_train):\n",
    "    raw_words = list(map(lambda word: re.sub(selector, '', word), review.split(' ')))\n",
    "    words = list(filter(lambda word: word.isalpha(), review.split(' ')))\n",
    "    if sentiments_train[index] == 'POSITIVE':\n",
    "        positive_word_count.update(words)\n",
    "    elif sentiments_train[index] == 'NEGATIVE':\n",
    "        negative_word_count.update(words)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Delete Words from Counters if not available in Vocabulary    \n",
    "del_keys = []\n",
    "for key, value in positive_word_count.items():\n",
    "    if vocab_dict[str(key)]!=None:\n",
    "        pass\n",
    "    else:\n",
    "        del_keys.append(positive_word_count[key])\n",
    "\n",
    "del_keys = []\n",
    "for key, value in positive_word_count.items():\n",
    "    if vocab_dict[str(key)]!=None:\n",
    "        pass\n",
    "    else:\n",
    "        del_keys.append(positive_word_count[key])\n",
    "\n",
    "for i in del_keys:\n",
    "    del negative_word_count[i]\n",
    "    \n",
    "total_counts = positive_word_count+negative_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintain a Counter object to store positive to negative ratios \n",
    "pos_neg_ratios = Counter()\n",
    "\n",
    "# Calculate the ratios of positive and negative uses of the most common words\n",
    "# Consider words to be \"common\" if they've been used at least 100 times\n",
    "for term,cnt in list(total_counts.most_common()):\n",
    "    if(cnt > 100):\n",
    "        try:\n",
    "            pos_neg_ratios[term] = (positive_word_count[term]+1)/(negative_word_count[term]+1)\n",
    "        except ZeroDivisionError:\n",
    "            pos_neg_ratios[term] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos-to-neg ratio for 'the' = 1.0618578344986418\n",
      "Pos-to-neg ratio for 'amazing' = 4.019607843137255\n",
      "Pos-to-neg ratio for 'terrible' = 0.17317612380250552\n",
      "Pos-to-neg ratio for 'movie' = 0.7571196940090633\n",
      "Pos-to-neg ratio for 'titaic' = 2.183333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Pos-to-neg ratio for 'the' = {}\".format(pos_neg_ratios[\"the\"]))\n",
    "print(\"Pos-to-neg ratio for 'amazing' = {}\".format(pos_neg_ratios[\"amazing\"]))\n",
    "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))\n",
    "\n",
    "print(\"Pos-to-neg ratio for 'movie' = {}\".format(pos_neg_ratios[\"movie\"]))\n",
    "print(\"Pos-to-neg ratio for 'titaic' = {}\".format(pos_neg_ratios[\"titanic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3777812959604037"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a log of the ratio\n",
    "for word,ratio in pos_neg_ratios.most_common():\n",
    "    pos_neg_ratios[word] = np.log(ratio)\n",
    "pos_neg_ratios['bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.269697449699962, 4.700480365792417)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the distribution of the log-ratio scores\n",
    "scores = np.array(list(pos_neg_ratios.values()))\n",
    "vocab_selected = list(pos_neg_ratios.keys())\n",
    "vocab_selected_1 = list(pos_neg_ratios.keys())\n",
    "\n",
    "h = plt.hist(scores,bins=20)\n",
    "min(scores), max(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above histogram should give you an idea about the distribution of the scores.\n",
    "\n",
    "Notice how the scores are distributed around 0. A word with score 0 can be considered as `neutral`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realize\n",
      "hands\n",
      "extreme\n",
      "beat\n",
      "onto\n",
      "4116\n"
     ]
    }
   ],
   "source": [
    "# Print few words with neutral score\n",
    "for ind in np.where(scores == 0)[0][0:5]:\n",
    "    print(vocab_selected[ind])\n",
    "\n",
    "print(len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**APPROACH 1** A simple non-machine learning that only uses the log-ratios to determine if a review is positive or negative. This function will be applied to the test data to calculate the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonml_classifier(review,pos_neg_ratios):\n",
    "    '''\n",
    "    Function that determines the sentiment for a given review.\n",
    "    \n",
    "    Inputs:\n",
    "      review - A text containing a movie review\n",
    "      pos_neg_ratios - A Counter object containing frequent words\n",
    "                       and corresponding log positive-negative ratio\n",
    "    Return:\n",
    "      sentiment - 'NEGATIVE' or 'POSITIVE'\n",
    "    '''\n",
    "    selector = '[^A-Za-z]+'\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = list(map(lambda word: re.sub(selector, '', word), review.split(' ')))\n",
    "    score = 0\n",
    "    for word in words:\n",
    "        # Threshold from analyzing words between in lower end of pos & neg words\n",
    "        if pos_neg_ratios[word] >= 0.5 or pos_neg_ratios[word] <= -0.5 and word not in stop_words:\n",
    "            score += pos_neg_ratios[word]\n",
    "    return 'NEGATIVE' if score<0 else 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model = 0.811\n"
     ]
    }
   ],
   "source": [
    "predictions_test = []\n",
    "for r in reviews_test:\n",
    "    l = nonml_classifier(r,pos_neg_ratios)\n",
    "    predictions_test.append(l)\n",
    "# calculate accuracy\n",
    "correct = 0\n",
    "for l,p in zip(sentiments_test,predictions_test):\n",
    "    if l == p:\n",
    "        correct = correct + 1\n",
    "print('Accuracy of the model = {}'.format((correct/len(sentiments_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2** Implemented a neural network for sentiment classification. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_vector(review,word2index):\n",
    "    '''\n",
    "    Function to count how many times each word is used in the given review,\n",
    "    # and then store those counts at the appropriate indices inside x.\n",
    "    '''\n",
    "    vocab_size = len(word2index)\n",
    "    x = np.zeros((1, vocab_size))\n",
    "    for w in review.split(' '):\n",
    "        if w in word2index.keys():\n",
    "            x[0][word2index[w]] += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ignore_words(pos_neg_ratios):\n",
    "    '''\n",
    "    Function to identify words to ignore from the vocabulary\n",
    "    '''\n",
    "    ignore_words = []\n",
    "    for words, ratios in pos_neg_ratios.items():\n",
    "        if (ratios <= 0.5) and (ratios >= -0.5):\n",
    "            ignore_words.append(words)\n",
    "    return ignore_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word2index mapping from word to an integer index\n",
    "word2index, word2index_or = {}, {}\n",
    "ignore_words = find_ignore_words(pos_neg_ratios)\n",
    "\n",
    "vocab_original = vocab_selected_1\n",
    "for i,word in enumerate(vocab_original):\n",
    "    word2index_or[word] = i\n",
    "vocab_size_or = (word2index_or)\n",
    "        \n",
    "vocab_selected = list(set(vocab_selected).difference(set(ignore_words)))\n",
    "for i,word in enumerate(vocab_selected):\n",
    "    if word not in ignore_words:\n",
    "        word2index[word] = i\n",
    "vocab_size = len(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate .hdf5 files from the processed data\n",
    "Given that the data is moderately large sized, the `hdf5` file format provides a more efficient file representation for further processing. See [here](https://anaconda.org/anaconda/hdf5) for more details and installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Without Ignore Words\n",
    "word2index = word2index_or\n",
    "vocab_size = len(vocab_size_or)\n",
    "'''\n",
    "\n",
    "# Run the script once to generate the file \n",
    "# delete the existing 'data1.hdf5' file before running it again to avoid error \n",
    "labels_train = np.zeros((len(sentiments_train), 2), dtype=int)\n",
    "labels_test = np.zeros((len(sentiments_test), 2), dtype=int)\n",
    "\n",
    "with h5py.File('data1.hdf5', 'w') as hf:\n",
    "    hf.create_dataset('data_train', (labels_train.shape[0], vocab_size), np.int16)\n",
    "    hf.create_dataset('data_test', (labels_test.shape[0], vocab_size), np.int16)\n",
    "    \n",
    "    # create training data\n",
    "    for i,(r,l) in enumerate(zip(reviews_train, sentiments_train)):\n",
    "        hf[\"data_train\"][i] = create_input_vector(r,word2index)\n",
    "        # one-hot encoding\n",
    "        if l == 'NEGATIVE':\n",
    "            labels_train[i, 0] = 1\n",
    "        else:\n",
    "            labels_train[i, 1] = 1\n",
    "            \n",
    "    # create test data\n",
    "    for i,(r,l) in enumerate(zip(reviews_test, sentiments_test)):\n",
    "        hf[\"data_test\"][i] = create_input_vector(r,word2index)\n",
    "        # one-hot encoding\n",
    "        if l == 'NEGATIVE':\n",
    "            labels_test[i, 0] = 1\n",
    "        else:\n",
    "            labels_test[i, 1] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "tf.random.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the network\n",
    "learning_rate = 0.01\n",
    "batch_size = 400\n",
    "num_epochs = 50\n",
    "n_input = vocab_size\n",
    "n_classes = 2\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights and biases in Tensorflow according to the parameters set above\n",
    "n_hidden_1 = 10  # 1st layer number of neurons\n",
    "n_hidden_2 = 8\n",
    "n_hidden_3 = 5\n",
    "\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "\t'out2': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "\t'out2': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "def multilayer_perceptron(x):\n",
    "    # define the layers of a single layer perceptron\n",
    "    net_layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    net_layer_2 = tf.add(tf.matmul(net_layer_1, weights['h2']), biases['b2'])\n",
    "    net_layer_3 = tf.add(tf.matmul(net_layer_2, weights['h3']), biases['b3'])\n",
    "    out_layer = tf.nn.sigmoid(tf.matmul(net_layer_3, weights['out2']) + biases['out2'])\n",
    "    return out_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = multilayer_perceptron(X)\n",
    "diff = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y)\n",
    "loss_op = tf.reduce_mean(diff)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some macosx installations, conflicting copies of mpilib causes trouble with tensorflow.\n",
    "# use the following two lines to resolve that issue\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.603542, Test_acc: 0.692500\n",
      "Train acc: 0.755250, Test_acc: 0.761250\n",
      "Train acc: 0.808958, Test_acc: 0.792500\n",
      "Train acc: 0.831458, Test_acc: 0.816250\n",
      "Train acc: 0.844000, Test_acc: 0.812500\n",
      "Train acc: 0.856917, Test_acc: 0.816250\n",
      "Train acc: 0.861917, Test_acc: 0.818750\n",
      "Train acc: 0.867250, Test_acc: 0.821250\n",
      "Train acc: 0.872500, Test_acc: 0.821250\n",
      "Train acc: 0.873417, Test_acc: 0.823750\n",
      "Train acc: 0.876000, Test_acc: 0.820000\n",
      "Train acc: 0.877083, Test_acc: 0.832500\n",
      "Train acc: 0.880208, Test_acc: 0.831250\n",
      "Train acc: 0.880875, Test_acc: 0.818750\n",
      "Train acc: 0.882333, Test_acc: 0.825000\n",
      "Train acc: 0.884667, Test_acc: 0.825000\n",
      "Train acc: 0.887208, Test_acc: 0.820000\n",
      "Train acc: 0.887667, Test_acc: 0.817500\n",
      "Train acc: 0.885792, Test_acc: 0.823750\n",
      "Train acc: 0.889292, Test_acc: 0.827500\n",
      "Train acc: 0.889917, Test_acc: 0.822500\n",
      "Train acc: 0.890042, Test_acc: 0.833750\n",
      "Train acc: 0.890833, Test_acc: 0.830000\n",
      "Train acc: 0.890917, Test_acc: 0.823750\n",
      "Train acc: 0.892792, Test_acc: 0.827500\n",
      "Train acc: 0.890083, Test_acc: 0.822500\n",
      "Train acc: 0.892042, Test_acc: 0.823750\n",
      "Train acc: 0.892292, Test_acc: 0.827500\n",
      "Train acc: 0.892583, Test_acc: 0.822500\n",
      "Train acc: 0.893958, Test_acc: 0.821250\n",
      "Train acc: 0.894583, Test_acc: 0.828750\n",
      "Train acc: 0.895542, Test_acc: 0.827500\n",
      "Train acc: 0.894375, Test_acc: 0.820000\n",
      "Train acc: 0.894958, Test_acc: 0.820000\n",
      "Train acc: 0.895042, Test_acc: 0.816250\n",
      "Train acc: 0.893792, Test_acc: 0.812500\n",
      "Train acc: 0.890792, Test_acc: 0.822500\n",
      "Train acc: 0.891417, Test_acc: 0.816250\n",
      "Train acc: 0.895458, Test_acc: 0.812500\n",
      "Train acc: 0.896333, Test_acc: 0.821250\n",
      "Train acc: 0.895042, Test_acc: 0.825000\n",
      "Train acc: 0.895042, Test_acc: 0.820000\n",
      "Train acc: 0.896250, Test_acc: 0.813750\n",
      "Train acc: 0.897417, Test_acc: 0.810000\n",
      "Train acc: 0.896792, Test_acc: 0.816250\n",
      "Train acc: 0.896083, Test_acc: 0.820000\n",
      "Train acc: 0.895833, Test_acc: 0.817500\n",
      "Train acc: 0.895792, Test_acc: 0.812500\n",
      "Train acc: 0.897458, Test_acc: 0.821250\n",
      "Train acc: 0.896625, Test_acc: 0.825000\n",
      "Time elapsed - 9.928737163543701 seconds.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "\n",
    "    h = h5py.File('data1.hdf5', 'r')\n",
    "    n1 = h.get('data_train') \n",
    "    n2 = h.get('data_test')\n",
    "\n",
    "    # Training cycle\n",
    "    total_batch_train = int(n1.shape[0] / batch_size)\n",
    "    total_batch_test = int(n2.shape[0] / batch_size)\n",
    "\n",
    "    for iter_num in range(num_epochs):\n",
    "        # variables for train and test accuracies\n",
    "        avg_acc_train = 0.\n",
    "        avg_acc_test = 0.\n",
    "        for i in range(total_batch_train):\n",
    "            train_x = n1[(i) * batch_size: (i + 1) * batch_size, ...]\n",
    "            train_y = labels_train[(i) * batch_size: (i + 1) * batch_size, :]\n",
    "            _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})#, options= run_options)\n",
    "\n",
    "            _label_train = [np.argmax(i) for i in _logits_train]\n",
    "            _label_train_y = [np.argmax(i) for i in train_y]\n",
    "            _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
    "            avg_acc_train += _accuracy_train\n",
    "        \n",
    "        for j in range(total_batch_test):\n",
    "            test_x = n2[(j) * batch_size: (j + 1) * batch_size, ...]\n",
    "            test_y = labels_test[(j) * batch_size: (j + 1) * batch_size, :]\n",
    "\n",
    "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
    "            _label_test = [np.argmax(i) for i in _logits_test]\n",
    "            _label_test_y = [np.argmax(i) for i in test_y]\n",
    "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
    "            avg_acc_test += _accuracy_test\n",
    "        \n",
    "        # print the train and test accuracies\n",
    "        print(\"Train acc: %f, Test_acc: %f\" % (avg_acc_train/total_batch_train, avg_acc_test/total_batch_test))\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print('Time elapsed - {} seconds.'.format(duration))    \n",
    "\n",
    "hf.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
